{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import regex as re\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_MODEL_DIR = \"mistralai/Mistral-7B-v0.1\"\n",
    "LOFTQ_MODEL_DIR = \"/home/ubuntu/peft/artifacts/loftq/Llama-2-7b-hf-4bit-64rank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa1eb747f9b4726a0ad0db4c782f0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m orig_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mORIG_MODEL_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/lplr/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/lplr/lib/python3.11/site-packages/transformers/modeling_utils.py:3706\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3698\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3699\u001b[0m     (\n\u001b[1;32m   3700\u001b[0m         model,\n\u001b[1;32m   3701\u001b[0m         missing_keys,\n\u001b[1;32m   3702\u001b[0m         unexpected_keys,\n\u001b[1;32m   3703\u001b[0m         mismatched_keys,\n\u001b[1;32m   3704\u001b[0m         offload_index,\n\u001b[1;32m   3705\u001b[0m         error_msgs,\n\u001b[0;32m-> 3706\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3713\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3714\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3717\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3718\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQuantizationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITS_AND_BYTES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3724\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3725\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/micromamba/envs/lplr/lib/python3.11/site-packages/transformers/modeling_utils.py:4134\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4132\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4134\u001b[0m     error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4136\u001b[0m \u001b[38;5;66;03m# force memory release\u001b[39;00m\n\u001b[1;32m   4137\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/micromamba/envs/lplr/lib/python3.11/site-packages/transformers/modeling_utils.py:606\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    604\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 606\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m~/micromamba/envs/lplr/lib/python3.11/site-packages/transformers/modeling_utils.py:604\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 604\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/lplr/lib/python3.11/site-packages/transformers/modeling_utils.py:604\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 604\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 604 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/micromamba/envs/lplr/lib/python3.11/site-packages/transformers/modeling_utils.py:604\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 604\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/lplr/lib/python3.11/site-packages/transformers/modeling_utils.py:600\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    598\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/lplr/lib/python3.11/site-packages/torch/nn/modules/module.py:2040\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2040\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   2042\u001b[0m     error_msgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhile copying the parameter named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2043\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the model are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2044\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the checkpoint are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_param\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2045\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124man exception occurred : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2046\u001b[0m                       )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "orig_model = AutoModelForCausalLM.from_pretrained(\n",
    "    ORIG_MODEL_DIR, \n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_model = orig_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8741a5cd1f6d4a79b4541555513b6d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-18 19:34:09.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.417\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.615\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.738\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.770\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:09.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.068\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.222\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.535\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.568\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.691\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.928\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:10.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.083\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.175\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: q_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.470\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: k_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: v_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.489\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: o_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: gate_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: up_proj\u001b[0m\n",
      "\u001b[32m2024-01-18 19:34:11.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpeft.tuners.lora.model\u001b[0m:\u001b[36m_create_and_replace\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mAdapting layer: down_proj\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LOFTQ_MODEL_DIR, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=False,\n",
    "        # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        # bnb_4bit_use_double_quant=False,\n",
    "        # bnb_4bit_quant_type='nf4',\n",
    "    ),\n",
    ")\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    LOFTQ_MODEL_DIR,\n",
    "    subfolder=\"loft_init\",\n",
    "    is_trainable=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, L, R = None, None, None\n",
    "layer_name = None\n",
    "\n",
    "orig_layers = orig_model.named_parameters()\n",
    "\n",
    "errors = {}\n",
    "\n",
    "for name, weight in peft_model.named_parameters():\n",
    "    if layer_name is None:\n",
    "        match = re.findall(r'(layers.*)\\.(.*)\\.base_layer\\.weight', name)\n",
    "        if match:\n",
    "            layer_name = match[0]\n",
    "            Q = weight\n",
    "        continue\n",
    "    # Get adapters\n",
    "    match = re.findall(r'(layers.*)\\.(.*)\\.default\\.weight', name)\n",
    "    if not match:\n",
    "        continue\n",
    "    layer_name_verify, adapter_name = match[0]\n",
    "    assert layer_name_verify == layer_name\n",
    "    \n",
    "    if adapter_name == 'lora_A':\n",
    "        R = weight\n",
    "    elif adapter_name == 'lora_B':\n",
    "        L = weight\n",
    "    else:\n",
    "        logger.warning(f'Unknown adapter type for layer {name} of the Peft model!')\n",
    "\n",
    "    if Q is not None and L is not None and R is not None:\n",
    "        X_hat = Q + L @ R\n",
    "        X = None\n",
    "        ## Now, time to find this layer in the original model\n",
    "        \n",
    "        orig_layer = next(orig_layers, None)\n",
    "        while orig_layer is not None:\n",
    "            orig_name, orig_weight = orig_layer\n",
    "            if layer_name in orig_name:\n",
    "                X = orig_weight\n",
    "                break\n",
    "        if X is None:\n",
    "            logger.warning(f'Could not find layer with substring {layer_name} in the original model!')\n",
    "            orig_layers = orig_model.named_parameters()\n",
    "        else:\n",
    "            relative_fro_err = torch.norm(X - X_hat, p='fro')\n",
    "            errors[layer_name] = relative_fro_err / torch.norm(X, p='fro')\n",
    "        \n",
    "        Q, L, R, layer_name = None, None, None, None\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
